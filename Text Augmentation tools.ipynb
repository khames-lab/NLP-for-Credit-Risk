{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b111ce40",
   "metadata": {},
   "source": [
    "# Text Augmentation Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18cede1",
   "metadata": {},
   "source": [
    "##### 1. Easy Data Augmentation tools: consist of four simple major actions decribed this bellow.\n",
    "- Synonym replacement : Replace n words in the sentence with synonyms from wordnet\n",
    "- Random deletion : Randomly delete words from the sentence with probability p\n",
    "- Randomly swap two words in the sentence n times \n",
    "- Random insertion : Randomly insert n words into the sentence\n",
    "\n",
    "##### 2. Back translation : is based on \n",
    "- Providing a sentence in a given source language (English).\n",
    "- Translating this sentence to an intermediate language (French for instance).\n",
    "- Re-Translating the French sentence back to the source language (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1829676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090cdea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\33627\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from random import shuffle\n",
    "random.seed(1)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "stop_words = stopwords.words('english')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf919bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_chars(line):\n",
    "    clean_line = \"\"\n",
    "    line = line.replace(\"â€™\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \")\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym)\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "    return new_words\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb1fb5",
   "metadata": {},
   "source": [
    "### 1.EDA: Easy Data Augmentation tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4206bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SR(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            #print(\"replaced\", random_word, \"with\", synonym)\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: #only replace up to n words\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "    new_words = sentence.split(' ')\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48674e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RD(words, p):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca8acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RS(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e558e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RI(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66505c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA(text, alpha_sr=0.2, alpha_ri=0.2, alpha_rs=0.2, p_rd=0.2, num_aug=1):\n",
    "    text = get_only_chars(text)\n",
    "    words = text.split(' ')\n",
    "    words = [word for word in words if word is not '']\n",
    "    num_words = len(words)\n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = int(num_aug/4)+1\n",
    "    #SR\n",
    "    if (alpha_sr > 0):\n",
    "        n_sr = max(1, int(alpha_sr*num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = SR(words, n_sr)\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "    #RI\n",
    "    if (alpha_ri > 0):\n",
    "        n_ri = max(1, int(alpha_ri*num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = RI(words, n_ri)\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "    #RS\n",
    "    if (alpha_rs > 0):\n",
    "        n_rs = max(1, int(alpha_rs*num_words))\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = RS(words, n_rs)\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "    #RD\n",
    "    if (p_rd > 0):\n",
    "        for _ in range(num_new_per_technique):\n",
    "            a_words = RD(words, p_rd)\n",
    "            augmented_sentences.append(' '.join(a_words))\n",
    "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "    shuffle(augmented_sentences)\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984ac8b",
   "metadata": {},
   "source": [
    "### 2.Back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f768b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en_fr = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "tokenizer_fr_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "model_en_fr = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "model_fr_en = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "def back_translate(text):   \n",
    "    tokenized_text_en_fr = tokenizer_en_fr.prepare_seq2seq_batch([text], return_tensors='pt')\n",
    "    translation_en_fr = model_en_fr.generate(**tokenized_text_en_fr )\n",
    "    fr_text = tokenizer_en_fr.batch_decode(translation_en_fr, skip_special_tokens=True)[0]\n",
    "    tokenized_text_fr_en = tokenizer_fr_en.prepare_seq2seq_batch([fr_text], return_tensors='pt')\n",
    "    translation_fr_en = model_fr_en.generate(**tokenized_text_fr_en )\n",
    "    en_text = tokenizer_fr_en.batch_decode(translation_fr_en, skip_special_tokens=True)[0]\n",
    "    return en_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ab501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello my friends! How are you doing today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eecf9fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my friends, how are you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_translate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78adf55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['friends my hello how are you doing today']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDA(text, alpha_sr=0.2, alpha_ri=0.2, alpha_rs=0.2, p_rd=0.2, num_aug=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
